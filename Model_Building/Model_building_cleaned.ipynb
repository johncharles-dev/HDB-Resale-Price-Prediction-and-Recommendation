{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDB Resale Price Prediction - Model Building\n",
    "\n",
    "This notebook covers the model building process for predicting HDB resale prices in Singapore using machine learning techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#1-setup-and-imports)\n",
    "2. [Data Loading and Exploration](#2-data-loading-and-exploration)\n",
    "3. [Feature Correlation Analysis](#3-feature-correlation-analysis)\n",
    "4. [Data Preparation](#4-data-preparation)\n",
    "5. [Model Training and Evaluation](#5-model-training-and-evaluation)\n",
    "6. [XGBoost Hyperparameter Tuning](#6-xgboost-hyperparameter-tuning)\n",
    "7. [Final Model Training and Export](#7-final-model-training-and-export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports <a id=\"1-setup-and-imports\"></a>\n",
    "\n",
    "Import all necessary libraries for data manipulation, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning - model selection and evaluation\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Machine learning - regression models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model persistence\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading and Exploration <a id=\"2-data-loading-and-exploration\"></a>\n",
    "\n",
    "Load the preprocessed HDB dataset and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed HDB resale dataset\n",
    "hdb_model = pd.read_csv('HDB_model_ready.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset structure and data types\n",
    "hdb_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for all features\n",
    "hdb_model.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Correlation Analysis <a id=\"3-feature-correlation-analysis\"></a>\n",
    "\n",
    "Visualize correlations between features to understand relationships and identify important predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns to put target variable (resale_price) first for better visualization\n",
    "cols = ['resale_price'] + [c for c in hdb_model.columns if c != 'resale_price']\n",
    "hdb_model = hdb_model[cols]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr = hdb_model.corr(numeric_only=True)\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    cmap='coolwarm',\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    square=True,\n",
    "    cbar_kws={'shrink': 0.8}\n",
    ")\n",
    "plt.title('HDB Features Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preparation <a id=\"4-data-preparation\"></a>\n",
    "\n",
    "Create dataset subsets for different modeling scenarios and define the train-test split strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset subsets for different analysis scenarios\n",
    "\n",
    "# Subset: Only common flat types (3-room, 4-room, 5-room)\n",
    "ds_imp_rooms = hdb_model[hdb_model['flat_type_int'].isin([3, 4, 5])]\n",
    "\n",
    "# Main dataset: All records\n",
    "ds_all = hdb_model.copy()\n",
    "\n",
    "# Subsets by region\n",
    "ds_region_0 = hdb_model[hdb_model['region_code'] == 0]  # Central\n",
    "ds_region_1 = hdb_model[hdb_model['region_code'] == 1]  # North/East\n",
    "ds_region_2 = hdb_model[hdb_model['region_code'] == 2]  # West\n",
    "\n",
    "# Common flat types split by region\n",
    "ds_imp_rooms_0 = ds_imp_rooms[ds_imp_rooms['region_code'] == 0]\n",
    "ds_imp_rooms_1 = ds_imp_rooms[ds_imp_rooms['region_code'] == 1]\n",
    "ds_imp_rooms_2 = ds_imp_rooms[ds_imp_rooms['region_code'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_split(df):\n",
    "    \"\"\"\n",
    "    Split dataset into train and test sets based on year.\n",
    "    Training: data before 2024\n",
    "    Testing: data from 2024 onwards\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame with 'year' and 'resale_price' columns\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    train = df[df['year'] < 2024]\n",
    "    test = df[df['year'] >= 2024]\n",
    "    \n",
    "    X_train = train.drop(columns=['resale_price'])\n",
    "    y_train = train['resale_price']\n",
    "    X_test = test.drop(columns=['resale_price'])\n",
    "    y_test = test['resale_price']\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Create train-test splits for all dataset variants\n",
    "dataset_names = [\n",
    "    'imp_rooms', 'imp_rooms_0', 'imp_rooms_1', 'imp_rooms_2',\n",
    "    'all', 'region_0', 'region_1', 'region_2'\n",
    "]\n",
    "\n",
    "dataset_list = [\n",
    "    ds_imp_rooms, ds_imp_rooms_0, ds_imp_rooms_1, ds_imp_rooms_2,\n",
    "    ds_all, ds_region_0, ds_region_1, ds_region_2\n",
    "]\n",
    "\n",
    "# Store all splits in a dictionary for easy access\n",
    "splits = {}\n",
    "for name, dataset in zip(dataset_names, dataset_list):\n",
    "    X_train, X_test, y_train, y_test = year_split(dataset)\n",
    "    splits[name] = (X_train, X_test, y_train, y_test)\n",
    "    print(f\"{name}: Train size = {len(X_train)}, Test size = {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Training and Evaluation <a id=\"5-model-training-and-evaluation\"></a>\n",
    "\n",
    "Train and evaluate three regression models:\n",
    "- **Linear Regression**: Simple baseline model\n",
    "- **Random Forest**: Ensemble of decision trees\n",
    "- **XGBoost**: Gradient boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models across all dataset splits\n",
    "results = {}\n",
    "\n",
    "for name, (X_train, X_test, y_train, y_test) in splits.items():\n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    model_results = {}\n",
    "    for model_name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        model_results[model_name] = {\n",
    "            'model': model,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'feature_importance': getattr(model, 'feature_importances_', None)\n",
    "        }\n",
    "    \n",
    "    results[name] = model_results\n",
    "\n",
    "# Display results for each dataset and model\n",
    "for ds_name, model_dict in results.items():\n",
    "    print(f\"\\n=== Results for dataset: {ds_name} ===\")\n",
    "    for m_name, res in model_dict.items():\n",
    "        print(f\"{m_name}: MAE = ${res['mae']:,.2f}, R² = {res['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. XGBoost Hyperparameter Tuning <a id=\"6-xgboost-hyperparameter-tuning\"></a>\n",
    "\n",
    "Use RandomizedSearchCV to find optimal hyperparameters for XGBoost on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/test split for the 'all' dataset\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = splits['all']\n",
    "\n",
    "# Define base XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],        # L1 regularization\n",
    "    'reg_lambda': [1, 1.5, 2]        # L2 regularization\n",
    "}\n",
    "\n",
    "# Perform randomized search with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,                        # Number of parameter combinations to try\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    cv=3,                             # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the search\n",
    "random_search.fit(X_train_all, y_train_all)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Best parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\nBest CV MAE: ${-random_search.best_score_:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model on test set\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "y_pred_all = best_xgb.predict(X_test_all)\n",
    "mae_all = mean_absolute_error(y_test_all, y_pred_all)\n",
    "r2_all = r2_score(y_test_all, y_pred_all)\n",
    "\n",
    "print(f\"Tuned XGBoost Test Performance:\")\n",
    "print(f\"  MAE: ${mae_all:,.2f}\")\n",
    "print(f\"  R²:  {r2_all:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Final Model Training and Export <a id=\"7-final-model-training-and-export\"></a>\n",
    "\n",
    "Train the final model on the complete dataset using optimized hyperparameters and save for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full dataset (train + test) for final model training\n",
    "X_full = ds_all.drop(columns=['resale_price'])\n",
    "y_full = ds_all['resale_price']\n",
    "\n",
    "# Get best parameters and ensure reproducibility settings\n",
    "best_params = random_search.best_params_.copy()\n",
    "best_params.update({'random_state': 42, 'n_jobs': -1})\n",
    "\n",
    "print(\"Training final model with parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Train final model on complete dataset\n",
    "final_xgb = xgb.XGBRegressor(**best_params)\n",
    "final_xgb.fit(X_full, y_full)\n",
    "\n",
    "print(f\"\\nFinal model trained on {len(X_full):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model for deployment\n",
    "model_path = 'xgb_resale_all.joblib'\n",
    "joblib.dump(final_xgb, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importances from the final model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_full.columns,\n",
    "    'importance': final_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('XGBoost Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
